{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from initial_plots import read_dataset\n",
    "import numpy as np\n",
    "from whole_pipeline import *\n",
    "from utils_torch import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_dataset(gt=False)\n",
    "\n",
    "X = np.array(df)\n",
    "X = X.reshape((610,340, 103))\n",
    "\n",
    "df = read_dataset(gt=True)\n",
    "y = np.array(df)\n",
    "\n",
    "rows_factor=21\n",
    "cols_factor=21\n",
    "\n",
    "rows_overlap=11 \n",
    "cols_overlap=11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version- using whole X as data\n",
    "# whole_pipeline_all(X,y, rows_factor, cols_factor, is_normalize_each_band=True, method_label_patch='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXX IN METHOD XXXXXXXXX\n",
      "PREPARE TIME:  5.5689005851745605\n",
      "HDE TIME:  8.864991664886475\n",
      "HDD TIME:  2.7823667526245117\n",
      "WHOLE METHOD TIME:  17.218811750411987\n",
      "XXXXXXX IN CLASSIFICATION XXXXXXXXX\n",
      "DICT CREATION, THROW 0 LABELS, SPLIT TETS TRAIN TIME:  0.005524873733520508\n",
      "patch_to_points_dict:  {0: (21, 42, 189, 210), 1: (21, 42, 210, 231), 2: (21, 42, 315, 336), 3: (42, 63, 210, 231), 4: (42, 63, 294, 315), 5: (63, 84, 126, 147), 6: (63, 84, 210, 231), 7: (63, 84, 273, 294), 8: (84, 105, 231, 252), 9: (84, 105, 252, 273), 10: (105, 126, 63, 84), 11: (126, 147, 21, 42), 12: (126, 147, 63, 84), 13: (126, 147, 84, 105), 14: (126, 147, 252, 273), 15: (147, 168, 42, 63), 16: (147, 168, 63, 84), 17: (147, 168, 189, 210), 18: (147, 168, 210, 231), 19: (168, 189, 21, 42), 20: (168, 189, 42, 63), 21: (168, 189, 63, 84), 22: (168, 189, 105, 126), 23: (168, 189, 147, 168), 24: (168, 189, 168, 189), 25: (168, 189, 189, 210), 26: (168, 189, 273, 294), 27: (168, 189, 315, 336), 28: (189, 210, 21, 42), 29: (189, 210, 42, 63), 30: (189, 210, 63, 84), 31: (189, 210, 105, 126), 32: (189, 210, 126, 147), 33: (189, 210, 147, 168), 34: (189, 210, 168, 189), 35: (189, 210, 294, 315), 36: (210, 231, 63, 84), 37: (210, 231, 84, 105), 38: (210, 231, 105, 126), 39: (210, 231, 126, 147), 40: (210, 231, 273, 294), 41: (231, 252, 63, 84), 42: (231, 252, 84, 105), 43: (231, 252, 315, 336), 44: (252, 273, 42, 63), 45: (252, 273, 315, 336), 46: (273, 294, 21, 42), 47: (273, 294, 63, 84), 48: (273, 294, 147, 168), 49: (273, 294, 336, 357), 50: (294, 315, 21, 42), 51: (294, 315, 63, 84), 52: (294, 315, 126, 147), 53: (294, 315, 147, 168), 54: (294, 315, 336, 357), 55: (315, 336, 42, 63), 56: (315, 336, 105, 126), 57: (315, 336, 147, 168), 58: (315, 336, 273, 294), 59: (336, 357, 21, 42), 60: (336, 357, 84, 105), 61: (336, 357, 147, 168), 62: (336, 357, 168, 189), 63: (336, 357, 231, 252), 64: (336, 357, 252, 273), 65: (336, 357, 357, 378), 66: (357, 378, 126, 147), 67: (357, 378, 168, 189), 68: (357, 378, 273, 294), 69: (378, 399, 63, 84), 70: (378, 399, 168, 189), 71: (378, 399, 189, 210), 72: (378, 399, 378, 399), 73: (399, 420, 105, 126), 74: (399, 420, 189, 210), 75: (399, 420, 273, 294), 76: (399, 420, 294, 315), 77: (399, 420, 378, 399), 78: (420, 441, 147, 168), 79: (420, 441, 189, 210), 80: (420, 441, 210, 231), 81: (420, 441, 252, 273), 82: (441, 462, 210, 231), 83: (441, 462, 294, 315), 84: (441, 462, 399, 420), 85: (462, 483, 168, 189), 86: (462, 483, 210, 231), 87: (462, 483, 231, 252), 88: (462, 483, 336, 357), 89: (483, 504, 63, 84), 90: (483, 504, 84, 105), 91: (483, 504, 147, 168), 92: (483, 504, 231, 252), 93: (483, 504, 294, 315), 94: (483, 504, 315, 336), 95: (504, 525, 63, 84), 96: (504, 525, 126, 147), 97: (504, 525, 189, 210), 98: (525, 546, 105, 126), 99: (525, 546, 168, 189), 100: (525, 546, 357, 378), 101: (525, 546, 378, 399), 102: (525, 546, 441, 462), 103: (525, 546, 462, 483), 104: (546, 567, 147, 168), 105: (546, 567, 336, 357), 106: (546, 567, 357, 378), 107: (546, 567, 378, 399), 108: (546, 567, 399, 420), 109: (546, 567, 483, 504), 110: (567, 588, 21, 42), 111: (567, 588, 189, 210), 112: (567, 588, 336, 357), 113: (567, 588, 357, 378), 114: (567, 588, 378, 399), 115: (567, 588, 399, 420), 116: (567, 588, 462, 483), 117: (588, 609, 273, 294), 118: (588, 609, 315, 336), 119: (588, 609, 357, 378), 120: (588, 609, 378, 399), 121: (588, 609, 399, 420), 122: (588, 609, 420, 441), 123: (609, 630, 42, 63), 124: (609, 630, 231, 252), 125: (609, 630, 252, 273), 126: (609, 630, 273, 294), 127: (609, 630, 336, 357), 128: (609, 630, 357, 378), 129: (609, 630, 378, 399), 130: (609, 630, 399, 420), 131: (609, 630, 420, 441), 132: (609, 630, 567, 588), 133: (630, 651, 21, 42), 134: (630, 651, 63, 84), 135: (630, 651, 231, 252), 136: (630, 651, 252, 273), 137: (630, 651, 315, 336), 138: (630, 651, 336, 357), 139: (630, 651, 357, 378), 140: (630, 651, 378, 399), 141: (630, 651, 399, 420), 142: (630, 651, 420, 441), 143: (630, 651, 441, 462), 144: (651, 672, 252, 273), 145: (651, 672, 315, 336), 146: (651, 672, 336, 357), 147: (651, 672, 357, 378), 148: (651, 672, 378, 399), 149: (651, 672, 399, 420), 150: (651, 672, 420, 441), 151: (651, 672, 441, 462), 152: (651, 672, 546, 567), 153: (672, 693, 252, 273), 154: (672, 693, 273, 294), 155: (672, 693, 315, 336), 156: (672, 693, 336, 357), 157: (672, 693, 357, 378), 158: (672, 693, 378, 399), 159: (672, 693, 399, 420), 160: (672, 693, 525, 546), 161: (693, 714, 42, 63), 162: (693, 714, 63, 84), 163: (693, 714, 273, 294), 164: (693, 714, 294, 315), 165: (693, 714, 336, 357), 166: (693, 714, 357, 378), 167: (693, 714, 378, 399), 168: (714, 735, 105, 126), 169: (714, 735, 210, 231), 170: (714, 735, 273, 294), 171: (714, 735, 546, 567), 172: (735, 756, 126, 147), 173: (735, 756, 630, 651), 174: (756, 777, 63, 84), 175: (756, 777, 105, 126), 176: (756, 777, 588, 609), 177: (777, 798, 147, 168), 178: (777, 798, 252, 273), 179: (777, 798, 567, 588), 180: (777, 798, 588, 609), 181: (798, 819, 42, 63), 182: (798, 819, 126, 147), 183: (798, 819, 273, 294), 184: (798, 819, 609, 630), 185: (819, 840, 147, 168), 186: (819, 840, 189, 210), 187: (840, 861, 21, 42), 188: (840, 861, 168, 189), 189: (840, 861, 231, 252), 190: (840, 861, 399, 420), 191: (840, 861, 483, 504), 192: (840, 861, 630, 651), 193: (861, 882, 315, 336), 194: (861, 882, 441, 462), 195: (882, 903, 21, 42), 196: (882, 903, 168, 189), 197: (882, 903, 420, 441), 198: (903, 924, 189, 210), 199: (903, 924, 420, 441), 200: (924, 945, 21, 42), 201: (924, 945, 105, 126), 202: (924, 945, 210, 231), 203: (924, 945, 273, 294), 204: (924, 945, 378, 399), 205: (945, 966, 105, 126), 206: (945, 966, 231, 252), 207: (945, 966, 399, 420), 208: (945, 966, 420, 441), 209: (966, 987, 63, 84), 210: (966, 987, 84, 105), 211: (966, 987, 147, 168), 212: (966, 987, 378, 399), 213: (966, 987, 399, 420), 214: (966, 987, 420, 441), 215: (966, 987, 441, 462), 216: (987, 1008, 21, 42), 217: (987, 1008, 168, 189), 218: (987, 1008, 231, 252), 219: (987, 1008, 336, 357), 220: (987, 1008, 378, 399), 221: (987, 1008, 399, 420), 222: (987, 1008, 420, 441), 223: (987, 1008, 441, 462), 224: (1008, 1029, 273, 294), 225: (1008, 1029, 315, 336), 226: (1008, 1029, 378, 399), 227: (1008, 1029, 399, 420), 228: (1008, 1029, 420, 441), 229: (1008, 1029, 441, 462), 230: (1008, 1029, 462, 483), 231: (1029, 1050, 105, 126), 232: (1029, 1050, 357, 378), 233: (1029, 1050, 378, 399), 234: (1029, 1050, 399, 420), 235: (1029, 1050, 420, 441), 236: (1029, 1050, 441, 462), 237: (1029, 1050, 462, 483), 238: (1050, 1071, 21, 42), 239: (1050, 1071, 126, 147), 240: (1050, 1071, 294, 315), 241: (1050, 1071, 357, 378), 242: (1050, 1071, 378, 399), 243: (1050, 1071, 399, 420), 244: (1050, 1071, 420, 441), 245: (1050, 1071, 441, 462), 246: (1050, 1071, 462, 483), 247: (1050, 1071, 483, 504), 248: (1071, 1092, 21, 42), 249: (1071, 1092, 42, 63), 250: (1071, 1092, 105, 126), 251: (1071, 1092, 147, 168), 252: (1071, 1092, 273, 294), 253: (1071, 1092, 336, 357), 254: (1071, 1092, 357, 378), 255: (1071, 1092, 378, 399), 256: (1071, 1092, 399, 420), 257: (1071, 1092, 420, 441), 258: (1071, 1092, 441, 462), 259: (1071, 1092, 462, 483), 260: (1071, 1092, 483, 504), 261: (1092, 1113, 21, 42), 262: (1092, 1113, 42, 63), 263: (1092, 1113, 63, 84), 264: (1092, 1113, 105, 126), 265: (1092, 1113, 189, 210), 266: (1092, 1113, 252, 273), 267: (1092, 1113, 273, 294), 268: (1092, 1113, 315, 336), 269: (1092, 1113, 336, 357), 270: (1092, 1113, 357, 378), 271: (1092, 1113, 378, 399), 272: (1092, 1113, 399, 420), 273: (1092, 1113, 420, 441), 274: (1092, 1113, 441, 462), 275: (1092, 1113, 462, 483), 276: (1092, 1113, 483, 504), 277: (1092, 1113, 504, 525), 278: (1113, 1134, 21, 42), 279: (1113, 1134, 42, 63), 280: (1113, 1134, 105, 126), 281: (1113, 1134, 126, 147), 282: (1113, 1134, 210, 231), 283: (1113, 1134, 294, 315), 284: (1113, 1134, 315, 336), 285: (1113, 1134, 336, 357), 286: (1113, 1134, 357, 378), 287: (1113, 1134, 378, 399), 288: (1113, 1134, 399, 420), 289: (1113, 1134, 420, 441), 290: (1113, 1134, 441, 462), 291: (1113, 1134, 462, 483), 292: (1113, 1134, 483, 504), 293: (1113, 1134, 504, 525), 294: (1134, 1155, 21, 42), 295: (1134, 1155, 42, 63), 296: (1134, 1155, 63, 84), 297: (1134, 1155, 231, 252), 298: (1134, 1155, 252, 273), 299: (1134, 1155, 273, 294), 300: (1134, 1155, 294, 315), 301: (1134, 1155, 315, 336), 302: (1134, 1155, 336, 357), 303: (1134, 1155, 357, 378), 304: (1134, 1155, 378, 399), 305: (1134, 1155, 399, 420), 306: (1134, 1155, 420, 441), 307: (1134, 1155, 441, 462), 308: (1134, 1155, 462, 483), 309: (1134, 1155, 483, 504), 310: (1134, 1155, 504, 525), 311: (1134, 1155, 525, 546), 312: (1155, 1176, 21, 42), 313: (1155, 1176, 42, 63), 314: (1155, 1176, 63, 84), 315: (1155, 1176, 84, 105), 316: (1155, 1176, 105, 126), 317: (1155, 1176, 126, 147), 318: (1155, 1176, 147, 168), 319: (1155, 1176, 168, 189), 320: (1155, 1176, 189, 210), 321: (1155, 1176, 210, 231), 322: (1155, 1176, 231, 252), 323: (1155, 1176, 252, 273), 324: (1155, 1176, 273, 294), 325: (1155, 1176, 294, 315), 326: (1155, 1176, 315, 336), 327: (1155, 1176, 336, 357), 328: (1155, 1176, 357, 378), 329: (1155, 1176, 378, 399), 330: (1155, 1176, 399, 420), 331: (1155, 1176, 420, 441), 332: (1155, 1176, 441, 462), 333: (1155, 1176, 462, 483), 334: (1155, 1176, 483, 504), 335: (1155, 1176, 504, 525), 336: (1155, 1176, 525, 546)}\n",
      "labels:  [1 4 1 4 1 4 1 4 1 2 2 1 2 2 2 2 2 2 4 2 4 2 2 2 2 2 2 1 2 2 2 2 4 2 2 1 2\n",
      " 1 2 1 2 1 1 2 2 8 8 5 1 8 2 2 5 5 1 8 2 5 8 2 2 1 8 2 5 1 8 2 5 2 1 8 2 2\n",
      " 5 4 1 8 2 5 4 8 1 6 6 4 8 6 6 6 6 4 3 8 6 6 6 6 1 7 7 6 6 6 6 7 7 6 6 6 3\n",
      " 3 7 6 6 6 6 7 6 6 6 6 6 6 6 4 7 6 6 6 6 1 3 3 7 7 6 6 3 1 3 3 1 3 1 1 3 3\n",
      " 1 3 9 9 3 8 9 1 1 1 1 8 8 4 2 2 8 8 4 2 2 2 2 2 8 2 2 2 8 4 2 2 2 2 2 1 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 4 1 8 1 2 2 2 2 2 2 2 4 1 4 4 1 4 2 2 2 2 2 2\n",
      " 2 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2]\n",
      "Number of patches:  269\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 362 is out of bounds for axis 1 with size 362",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# old version- using whole X as data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mwhole_pipeline_all_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_normalize_each_band\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_label_patch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcenter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ronen\\.vscode\\HyperbolicDiffusionDistance_midTerm\\paviaU\\utils_torch.py:337\u001b[0m, in \u001b[0;36mwhole_pipeline_all_torch\u001b[1;34m(X, y, rows_factor, cols_factor, rows_overlap, cols_overlap, is_normalize_each_band, method_label_patch)\u001b[0m\n\u001b[0;32m    333\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    335\u001b[0m y_patches \u001b[38;5;241m=\u001b[39m y_patches\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m--> 337\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_HDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_patches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_padded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_overlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_patches_in_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWHOLE CLASSIFICATION TIME: \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mst)\n",
      "File \u001b[1;32mc:\\Users\\ronen\\.vscode\\HyperbolicDiffusionDistance_midTerm\\paviaU\\classification.py:209\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(distances_mat, labels, n_neighbors, labels_padded, rows_factor, cols_factor, rows_overlap, cols_overlap, num_patches_in_row)\u001b[0m\n\u001b[0;32m    205\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(labels\u001b[38;5;241m=\u001b[39mlabels_train, patch_to_points_dict\u001b[38;5;241m=\u001b[39mpatch_to_points_dict)\n\u001b[0;32m    207\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 209\u001b[0m train_acc, train_preds,train_gt \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmat_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m test_acc, test_preds,test_gt\u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mscore(dmat_test, indices_test, labels_padded)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m,train_acc)\n",
      "File \u001b[1;32mc:\\Users\\ronen\\.vscode\\HyperbolicDiffusionDistance_midTerm\\paviaU\\classification.py:45\u001b[0m, in \u001b[0;36mkNN.score\u001b[1;34m(self, distances, indices_test, y)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i_start,i_end):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(j_start,j_end):\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m             total_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m y[i,j] \u001b[38;5;241m==\u001b[39m predictions[ind]:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 362 is out of bounds for axis 1 with size 362"
     ]
    }
   ],
   "source": [
    "# old version- using whole X as data\n",
    "whole_pipeline_all_torch(torch.from_numpy(X),torch.from_numpy(y), rows_factor, cols_factor, rows_overlap, cols_overlap, is_normalize_each_band=True, method_label_patch='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version- dividing X to spectral bands\n",
    "#whole_pipeline_divided(X,y, rows_factor, cols_factor, is_normalize_each_band=True, method_label_patch='center')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARE TIME:  0.467130184173584\n",
      "HDE TIME:  5.291319370269775\n",
      "HDD TIME:  1.7093675136566162\n",
      "PREPARE TIME:  0.42734289169311523\n",
      "HDE TIME:  6.144777297973633\n",
      "HDD TIME:  2.0702497959136963\n",
      "PREPARE TIME:  0.5110526084899902\n",
      "HDE TIME:  5.9933366775512695\n",
      "HDD TIME:  1.8296031951904297\n",
      "PREPARE TIME:  0.40571022033691406\n",
      "HDE TIME:  5.9665892124176025\n",
      "HDD TIME:  1.9367713928222656\n",
      "PREPARE TIME:  0.41564083099365234\n",
      "HDE TIME:  5.891500473022461\n",
      "HDD TIME:  1.853825330734253\n",
      "PREPARE TIME:  0.5148303508758545\n",
      "HDE TIME:  6.017426490783691\n",
      "HDD TIME:  1.9830970764160156\n",
      "PREPARE TIME:  0.47592949867248535\n",
      "HDE TIME:  7.126255512237549\n",
      "HDD TIME:  1.8391969203948975\n",
      "PREPARE TIME:  0.40160155296325684\n",
      "HDE TIME:  5.844698905944824\n",
      "HDD TIME:  1.940943956375122\n",
      "PREPARE TIME:  0.5088648796081543\n",
      "HDE TIME:  7.01116156578064\n",
      "HDD TIME:  1.7252857685089111\n",
      "PREPARE TIME:  0.42826318740844727\n",
      "HDE TIME:  5.599522590637207\n",
      "HDD TIME:  1.7001338005065918\n",
      "PREPARE TIME:  0.3710751533508301\n",
      "HDE TIME:  5.3700947761535645\n",
      "HDD TIME:  1.6798105239868164\n",
      "PREPARE TIME:  0.42977166175842285\n",
      "HDE TIME:  5.525232553482056\n",
      "HDD TIME:  1.8221421241760254\n",
      "PREPARE TIME:  0.396456241607666\n",
      "HDE TIME:  5.343569755554199\n",
      "HDD TIME:  1.6711509227752686\n",
      "PREPARE TIME:  0.4655015468597412\n",
      "HDE TIME:  5.239553213119507\n",
      "HDD TIME:  1.6335880756378174\n",
      "PREPARE TIME:  0.4198486804962158\n",
      "HDE TIME:  5.373153448104858\n",
      "HDD TIME:  1.6573803424835205\n",
      "PREPARE TIME:  0.3864004611968994\n",
      "HDE TIME:  5.335109710693359\n",
      "HDD TIME:  1.683427333831787\n",
      "PREPARE TIME:  0.39211344718933105\n",
      "HDE TIME:  5.703044891357422\n",
      "HDD TIME:  2.079986810684204\n",
      "PREPARE TIME:  0.508310079574585\n",
      "HDE TIME:  5.966939449310303\n",
      "HDD TIME:  1.6892118453979492\n",
      "PREPARE TIME:  0.4876236915588379\n",
      "HDE TIME:  5.650115251541138\n",
      "HDD TIME:  1.7733983993530273\n",
      "PREPARE TIME:  0.36928796768188477\n",
      "HDE TIME:  5.83453106880188\n",
      "HDD TIME:  1.7043797969818115\n",
      "PREPARE TIME:  0.43883204460144043\n",
      "HDE TIME:  5.697356224060059\n",
      "HDD TIME:  1.8587286472320557\n",
      "PREPARE TIME:  0.4040396213531494\n",
      "HDE TIME:  5.574151992797852\n",
      "HDD TIME:  1.6888213157653809\n",
      "PREPARE TIME:  0.44165635108947754\n",
      "HDE TIME:  5.608147621154785\n",
      "HDD TIME:  1.8233067989349365\n",
      "PREPARE TIME:  0.3745124340057373\n",
      "HDE TIME:  5.9488890171051025\n",
      "HDD TIME:  1.8890869617462158\n",
      "PREPARE TIME:  0.39985108375549316\n",
      "HDE TIME:  5.883955478668213\n",
      "HDD TIME:  1.6151726245880127\n",
      "PREPARE TIME:  0.3764922618865967\n",
      "HDE TIME:  5.421313047409058\n",
      "HDD TIME:  1.8852806091308594\n",
      "PREPARE TIME:  0.41191697120666504\n",
      "HDE TIME:  6.057453870773315\n",
      "HDD TIME:  1.9153857231140137\n",
      "PREPARE TIME:  0.534294843673706\n",
      "HDE TIME:  5.917720556259155\n",
      "HDD TIME:  1.8230748176574707\n",
      "PREPARE TIME:  0.39891791343688965\n",
      "HDE TIME:  5.387206792831421\n",
      "HDD TIME:  1.67641282081604\n",
      "PREPARE TIME:  0.3666856288909912\n",
      "HDE TIME:  4.893312454223633\n",
      "HDD TIME:  1.632049560546875\n",
      "PREPARE TIME:  0.35102272033691406\n",
      "HDE TIME:  4.999003648757935\n",
      "HDD TIME:  1.6179516315460205\n",
      "PREPARE TIME:  0.3740999698638916\n",
      "HDE TIME:  5.066150426864624\n",
      "HDD TIME:  1.6788506507873535\n",
      "PREPARE TIME:  0.36124181747436523\n",
      "HDE TIME:  4.963911771774292\n",
      "HDD TIME:  1.6427977085113525\n",
      "PREPARE TIME:  0.3518638610839844\n",
      "HDE TIME:  5.369976758956909\n",
      "HDD TIME:  1.9876797199249268\n",
      "PREPARE TIME:  0.4845569133758545\n",
      "HDE TIME:  5.488466739654541\n",
      "HDD TIME:  1.8083560466766357\n",
      "PREPARE TIME:  0.37801194190979004\n",
      "HDE TIME:  5.2300379276275635\n",
      "HDD TIME:  1.6435654163360596\n",
      "PREPARE TIME:  0.3485527038574219\n",
      "HDE TIME:  5.014973402023315\n",
      "HDD TIME:  1.6095409393310547\n",
      "PREPARE TIME:  0.34801435470581055\n",
      "HDE TIME:  5.509462356567383\n",
      "HDD TIME:  1.8341600894927979\n",
      "PREPARE TIME:  0.37804245948791504\n",
      "HDE TIME:  5.143353223800659\n",
      "HDD TIME:  1.607581377029419\n",
      "PREPARE TIME:  0.37158823013305664\n",
      "HDE TIME:  5.158856630325317\n",
      "HDD TIME:  1.817436933517456\n",
      "PREPARE TIME:  0.5398366451263428\n",
      "HDE TIME:  6.047648191452026\n",
      "HDD TIME:  1.7553999423980713\n",
      "PREPARE TIME:  0.356494665145874\n",
      "HDE TIME:  5.2658257484436035\n",
      "HDD TIME:  1.7192962169647217\n",
      "PREPARE TIME:  0.40780162811279297\n",
      "HDE TIME:  5.52004337310791\n",
      "HDD TIME:  1.6864371299743652\n",
      "PREPARE TIME:  0.3567819595336914\n",
      "HDE TIME:  5.03788161277771\n",
      "HDD TIME:  1.6689980030059814\n",
      "PREPARE TIME:  0.3487548828125\n",
      "HDE TIME:  4.929080247879028\n",
      "HDD TIME:  1.6539790630340576\n",
      "PREPARE TIME:  0.3548765182495117\n",
      "HDE TIME:  4.914912700653076\n",
      "HDD TIME:  1.6419038772583008\n",
      "PREPARE TIME:  0.3440670967102051\n",
      "HDE TIME:  4.85677433013916\n",
      "HDD TIME:  1.6542201042175293\n",
      "PREPARE TIME:  0.36182165145874023\n",
      "HDE TIME:  5.077580213546753\n",
      "HDD TIME:  1.6816117763519287\n",
      "PREPARE TIME:  0.38877105712890625\n",
      "HDE TIME:  5.008631467819214\n",
      "HDD TIME:  1.6722455024719238\n",
      "PREPARE TIME:  0.35779571533203125\n",
      "HDE TIME:  4.913431882858276\n",
      "HDD TIME:  1.6735494136810303\n",
      "PREPARE TIME:  0.37366366386413574\n",
      "HDE TIME:  5.027178525924683\n",
      "HDD TIME:  1.6140472888946533\n",
      "PREPARE TIME:  0.3430147171020508\n",
      "HDE TIME:  4.906856298446655\n",
      "HDD TIME:  1.6362473964691162\n",
      "PREPARE TIME:  0.33136749267578125\n",
      "HDE TIME:  4.994497537612915\n",
      "HDD TIME:  1.628584623336792\n",
      "PREPARE TIME:  0.34383535385131836\n",
      "HDE TIME:  4.855095148086548\n",
      "HDD TIME:  1.6510884761810303\n",
      "PREPARE TIME:  0.3698275089263916\n",
      "HDE TIME:  4.91195821762085\n",
      "HDD TIME:  1.6087756156921387\n",
      "PREPARE TIME:  0.39041709899902344\n",
      "HDE TIME:  4.940933704376221\n",
      "HDD TIME:  1.6469347476959229\n",
      "PREPARE TIME:  0.341494083404541\n",
      "HDE TIME:  4.958155155181885\n",
      "HDD TIME:  1.6718475818634033\n",
      "PREPARE TIME:  0.3533337116241455\n",
      "HDE TIME:  4.947011232376099\n",
      "HDD TIME:  1.68574857711792\n",
      "PREPARE TIME:  0.34490466117858887\n",
      "HDE TIME:  4.99922251701355\n",
      "HDD TIME:  1.6815578937530518\n",
      "PREPARE TIME:  0.3772621154785156\n",
      "HDE TIME:  5.022800445556641\n",
      "HDD TIME:  1.6540319919586182\n",
      "PREPARE TIME:  0.35609006881713867\n",
      "HDE TIME:  5.018025875091553\n",
      "HDD TIME:  1.6804442405700684\n",
      "PREPARE TIME:  0.35837817192077637\n",
      "HDE TIME:  5.12377142906189\n",
      "HDD TIME:  1.6441600322723389\n",
      "PREPARE TIME:  0.37680912017822266\n",
      "HDE TIME:  4.961575031280518\n",
      "HDD TIME:  1.629380226135254\n",
      "PREPARE TIME:  0.3749876022338867\n",
      "HDE TIME:  5.04403829574585\n",
      "HDD TIME:  1.6146206855773926\n",
      "PREPARE TIME:  0.40186429023742676\n",
      "HDE TIME:  5.003332614898682\n",
      "HDD TIME:  1.6432695388793945\n",
      "PREPARE TIME:  0.37628698348999023\n",
      "HDE TIME:  4.993960857391357\n",
      "HDD TIME:  1.6778326034545898\n",
      "PREPARE TIME:  0.34659457206726074\n",
      "HDE TIME:  5.043600082397461\n",
      "HDD TIME:  1.6398289203643799\n",
      "PREPARE TIME:  0.3395552635192871\n",
      "HDE TIME:  4.936773061752319\n",
      "HDD TIME:  1.653031349182129\n",
      "PREPARE TIME:  0.3253962993621826\n",
      "HDE TIME:  5.077417612075806\n",
      "HDD TIME:  1.6877222061157227\n",
      "PREPARE TIME:  0.3476986885070801\n",
      "HDE TIME:  5.025392532348633\n",
      "HDD TIME:  1.7777862548828125\n",
      "PREPARE TIME:  0.4716758728027344\n",
      "HDE TIME:  5.113576650619507\n",
      "HDD TIME:  1.6403303146362305\n",
      "PREPARE TIME:  0.37391233444213867\n",
      "HDE TIME:  5.306331396102905\n",
      "HDD TIME:  1.7573435306549072\n",
      "PREPARE TIME:  0.3692591190338135\n",
      "HDE TIME:  5.742895603179932\n",
      "HDD TIME:  1.743037223815918\n",
      "PREPARE TIME:  0.4564354419708252\n",
      "HDE TIME:  5.676512002944946\n",
      "HDD TIME:  1.8884882926940918\n",
      "PREPARE TIME:  0.4272434711456299\n",
      "HDE TIME:  5.246962547302246\n",
      "HDD TIME:  1.7097148895263672\n",
      "PREPARE TIME:  0.3727579116821289\n",
      "HDE TIME:  5.082620143890381\n",
      "HDD TIME:  1.7428913116455078\n",
      "PREPARE TIME:  0.3634641170501709\n",
      "HDE TIME:  5.300402641296387\n",
      "HDD TIME:  1.760019063949585\n",
      "PREPARE TIME:  0.39064788818359375\n",
      "HDE TIME:  5.35828971862793\n",
      "HDD TIME:  1.8078064918518066\n",
      "PREPARE TIME:  0.38150548934936523\n",
      "HDE TIME:  5.49188756942749\n",
      "HDD TIME:  1.6811633110046387\n",
      "PREPARE TIME:  0.3709244728088379\n",
      "HDE TIME:  5.3030195236206055\n",
      "HDD TIME:  1.8562748432159424\n",
      "PREPARE TIME:  0.43390870094299316\n",
      "HDE TIME:  5.394787073135376\n",
      "HDD TIME:  1.7886102199554443\n",
      "PREPARE TIME:  0.45306944847106934\n",
      "HDE TIME:  5.400777816772461\n",
      "HDD TIME:  1.7555968761444092\n",
      "PREPARE TIME:  0.42169857025146484\n",
      "HDE TIME:  5.2172064781188965\n",
      "HDD TIME:  1.7703089714050293\n",
      "PREPARE TIME:  0.4078078269958496\n",
      "HDE TIME:  5.258782863616943\n",
      "HDD TIME:  1.9031779766082764\n",
      "PREPARE TIME:  0.4520385265350342\n",
      "HDE TIME:  5.72021746635437\n",
      "HDD TIME:  2.2149364948272705\n",
      "PREPARE TIME:  0.5528907775878906\n",
      "HDE TIME:  5.974717378616333\n",
      "HDD TIME:  1.862020492553711\n",
      "PREPARE TIME:  0.3912084102630615\n",
      "HDE TIME:  5.709707975387573\n",
      "HDD TIME:  1.7732648849487305\n",
      "PREPARE TIME:  0.34826183319091797\n",
      "HDE TIME:  5.162286996841431\n",
      "HDD TIME:  1.690798282623291\n",
      "PREPARE TIME:  0.36809563636779785\n",
      "HDE TIME:  5.159549236297607\n",
      "HDD TIME:  1.7393269538879395\n",
      "PREPARE TIME:  0.35328006744384766\n",
      "HDE TIME:  5.414496183395386\n",
      "HDD TIME:  1.7445588111877441\n",
      "PREPARE TIME:  0.34935975074768066\n",
      "HDE TIME:  5.1831207275390625\n",
      "HDD TIME:  1.7492284774780273\n",
      "PREPARE TIME:  0.4113771915435791\n",
      "HDE TIME:  5.537336587905884\n",
      "HDD TIME:  1.6595056056976318\n",
      "PREPARE TIME:  0.39306116104125977\n",
      "HDE TIME:  5.140817642211914\n",
      "HDD TIME:  1.703704595565796\n",
      "PREPARE TIME:  0.36576080322265625\n",
      "HDE TIME:  5.310087203979492\n",
      "HDD TIME:  2.341879367828369\n",
      "PREPARE TIME:  0.5079386234283447\n",
      "HDE TIME:  5.2800562381744385\n",
      "HDD TIME:  1.766061782836914\n",
      "PREPARE TIME:  0.4054877758026123\n",
      "HDE TIME:  5.346649169921875\n",
      "HDD TIME:  1.7154638767242432\n",
      "PREPARE TIME:  0.3956151008605957\n",
      "HDE TIME:  5.412296772003174\n",
      "HDD TIME:  1.749617338180542\n",
      "PREPARE TIME:  0.39343762397766113\n",
      "HDE TIME:  5.477798223495483\n",
      "HDD TIME:  2.047994375228882\n",
      "PREPARE TIME:  0.4689755439758301\n",
      "HDE TIME:  5.302245616912842\n",
      "HDD TIME:  1.7543413639068604\n",
      "PREPARE TIME:  0.3722391128540039\n",
      "HDE TIME:  5.195787668228149\n",
      "HDD TIME:  1.8242793083190918\n",
      "PREPARE TIME:  0.4500718116760254\n",
      "HDE TIME:  5.274682521820068\n",
      "HDD TIME:  1.7474489212036133\n",
      "PREPARE TIME:  0.3986845016479492\n",
      "HDE TIME:  6.061246633529663\n",
      "HDD TIME:  2.015002727508545\n",
      "PREPARE TIME:  0.5460422039031982\n",
      "HDE TIME:  6.863134384155273\n",
      "HDD TIME:  1.982891321182251\n",
      "TOTAL TIME FOR METHOD:  778.0953783988953\n",
      "patch_to_points_dict:  {0: (0, 12, 96, 108), 1: (0, 12, 108, 120), 2: (0, 12, 168, 180), 3: (12, 24, 108, 120), 4: (12, 24, 156, 168), 5: (24, 36, 72, 84), 6: (24, 36, 108, 120), 7: (24, 36, 144, 156), 8: (36, 48, 48, 60), 9: (36, 48, 120, 132), 10: (36, 48, 132, 144), 11: (48, 60, 120, 132), 12: (60, 72, 36, 48), 13: (60, 72, 48, 60), 14: (60, 72, 132, 144), 15: (60, 72, 144, 156), 16: (72, 84, 24, 36), 17: (72, 84, 96, 108), 18: (72, 84, 108, 120), 19: (72, 84, 132, 144), 20: (84, 96, 0, 12), 21: (84, 96, 12, 24), 22: (84, 96, 24, 36), 23: (84, 96, 72, 84), 24: (84, 96, 84, 96), 25: (84, 96, 96, 108), 26: (84, 96, 144, 156), 27: (96, 108, 0, 12), 28: (96, 108, 12, 24), 29: (96, 108, 24, 36), 30: (96, 108, 48, 60), 31: (96, 108, 60, 72), 32: (96, 108, 72, 84), 33: (108, 120, 24, 36), 34: (108, 120, 36, 48), 35: (108, 120, 48, 60), 36: (108, 120, 144, 156), 37: (120, 132, 24, 36), 38: (120, 132, 36, 48), 39: (132, 144, 12, 24), 40: (132, 144, 36, 48), 41: (132, 144, 168, 180), 42: (144, 156, 12, 24), 43: (144, 156, 72, 84), 44: (144, 156, 132, 144), 45: (156, 168, 0, 12), 46: (156, 168, 12, 24), 47: (156, 168, 36, 48), 48: (156, 168, 48, 60), 49: (156, 168, 72, 84), 50: (156, 168, 108, 120), 51: (156, 168, 120, 132), 52: (156, 168, 180, 192), 53: (168, 180, 0, 12), 54: (168, 180, 60, 72), 55: (168, 180, 72, 84), 56: (168, 180, 84, 96), 57: (168, 180, 180, 192), 58: (180, 192, 12, 24), 59: (180, 192, 60, 72), 60: (180, 192, 84, 96), 61: (180, 192, 144, 156), 62: (180, 192, 192, 204), 63: (192, 204, 48, 60), 64: (192, 204, 84, 96), 65: (192, 204, 96, 108), 66: (192, 204, 120, 132), 67: (192, 204, 192, 204), 68: (204, 216, 36, 48), 69: (204, 216, 96, 108), 70: (204, 216, 144, 156), 71: (216, 228, 24, 36), 72: (216, 228, 96, 108), 73: (216, 228, 108, 120), 74: (228, 240, 108, 120), 75: (228, 240, 144, 156), 76: (228, 240, 204, 216), 77: (240, 252, 36, 48), 78: (240, 252, 108, 120), 79: (240, 252, 120, 132), 80: (240, 252, 168, 180), 81: (240, 252, 216, 228), 82: (264, 276, 84, 96), 83: (264, 276, 192, 204), 84: (264, 276, 228, 240), 85: (276, 288, 72, 84), 86: (276, 288, 108, 120), 87: (276, 288, 168, 180), 88: (276, 288, 180, 192), 89: (276, 288, 192, 204), 90: (276, 288, 204, 216), 91: (288, 300, 168, 180), 92: (288, 300, 180, 192), 93: (288, 300, 192, 204), 94: (288, 300, 204, 216), 95: (288, 300, 240, 252), 96: (288, 300, 312, 324), 97: (300, 312, 12, 24), 98: (300, 312, 60, 72), 99: (300, 312, 144, 156), 100: (300, 312, 180, 192), 101: (300, 312, 192, 204), 102: (300, 312, 204, 216), 103: (300, 312, 216, 228), 104: (312, 324, 120, 132), 105: (312, 324, 132, 144), 106: (312, 324, 144, 156), 107: (312, 324, 156, 168), 108: (312, 324, 180, 192), 109: (312, 324, 192, 204), 110: (312, 324, 204, 216), 111: (312, 324, 216, 228), 112: (312, 324, 252, 264), 113: (324, 336, 12, 24), 114: (324, 336, 24, 36), 115: (324, 336, 108, 120), 116: (324, 336, 120, 132), 117: (324, 336, 156, 168), 118: (324, 336, 168, 180), 119: (324, 336, 180, 192), 120: (324, 336, 192, 204), 121: (324, 336, 204, 216), 122: (324, 336, 216, 228), 123: (324, 336, 228, 240), 124: (336, 348, 168, 180), 125: (336, 348, 180, 192), 126: (336, 348, 192, 204), 127: (336, 348, 204, 216), 128: (336, 348, 216, 228), 129: (336, 348, 228, 240), 130: (336, 348, 264, 276), 131: (336, 348, 276, 288), 132: (348, 360, 144, 156), 133: (348, 360, 168, 180), 134: (348, 360, 180, 192), 135: (348, 360, 192, 204), 136: (348, 360, 204, 216), 137: (360, 372, 12, 24), 138: (360, 372, 36, 48), 139: (360, 372, 132, 144), 140: (360, 372, 144, 156), 141: (360, 372, 156, 168), 142: (360, 372, 180, 192), 143: (360, 372, 276, 288), 144: (372, 384, 60, 72), 145: (372, 384, 276, 288), 146: (372, 384, 288, 300), 147: (372, 384, 324, 336), 148: (372, 384, 336, 348), 149: (384, 396, 120, 132), 150: (396, 408, 72, 84), 151: (396, 408, 300, 312), 152: (408, 420, 12, 24), 153: (408, 420, 60, 72), 154: (408, 420, 288, 300), 155: (408, 420, 312, 324), 156: (420, 432, 72, 84), 157: (420, 432, 120, 132), 158: (420, 432, 144, 156), 159: (420, 432, 228, 240), 160: (432, 444, 12, 24), 161: (432, 444, 72, 84), 162: (432, 444, 120, 132), 163: (432, 444, 252, 264), 164: (432, 444, 324, 336), 165: (444, 456, 0, 12), 166: (444, 456, 144, 156), 167: (444, 456, 192, 204), 168: (444, 456, 336, 348), 169: (456, 468, 12, 24), 170: (456, 468, 84, 96), 171: (456, 468, 144, 156), 172: (456, 468, 168, 180), 173: (456, 468, 216, 228), 174: (468, 480, 12, 24), 175: (468, 480, 96, 108), 176: (468, 480, 168, 180), 177: (468, 480, 180, 192), 178: (480, 492, 12, 24), 179: (480, 492, 48, 60), 180: (480, 492, 132, 144), 181: (492, 504, 48, 60), 182: (492, 504, 84, 96), 183: (492, 504, 144, 156), 184: (492, 504, 204, 216), 185: (492, 504, 216, 228), 186: (492, 504, 228, 240), 187: (504, 516, 168, 180), 188: (504, 516, 192, 204), 189: (504, 516, 204, 216), 190: (504, 516, 216, 228), 191: (504, 516, 228, 240), 192: (516, 528, 108, 120), 193: (516, 528, 132, 144), 194: (516, 528, 156, 168), 195: (516, 528, 192, 204), 196: (516, 528, 204, 216), 197: (516, 528, 216, 228), 198: (516, 528, 228, 240), 199: (528, 540, 0, 12), 200: (528, 540, 60, 72), 201: (528, 540, 120, 132), 202: (528, 540, 144, 156), 203: (528, 540, 192, 204), 204: (528, 540, 204, 216), 205: (528, 540, 216, 228), 206: (528, 540, 228, 240), 207: (528, 540, 240, 252), 208: (540, 552, 36, 48), 209: (540, 552, 72, 84), 210: (540, 552, 180, 192), 211: (540, 552, 192, 204), 212: (540, 552, 204, 216), 213: (540, 552, 216, 228), 214: (540, 552, 228, 240), 215: (540, 552, 240, 252), 216: (552, 564, 144, 156), 217: (552, 564, 168, 180), 218: (552, 564, 180, 192), 219: (552, 564, 192, 204), 220: (552, 564, 204, 216), 221: (552, 564, 216, 228), 222: (552, 564, 228, 240), 223: (552, 564, 240, 252), 224: (552, 564, 252, 264), 225: (564, 576, 0, 12), 226: (564, 576, 96, 108), 227: (564, 576, 108, 120), 228: (564, 576, 132, 144), 229: (564, 576, 168, 180), 230: (564, 576, 180, 192), 231: (564, 576, 192, 204), 232: (564, 576, 204, 216), 233: (564, 576, 216, 228), 234: (564, 576, 228, 240), 235: (564, 576, 240, 252), 236: (564, 576, 252, 264), 237: (576, 588, 0, 12), 238: (576, 588, 12, 24), 239: (576, 588, 48, 60), 240: (576, 588, 60, 72), 241: (576, 588, 108, 120), 242: (576, 588, 156, 168), 243: (576, 588, 168, 180), 244: (576, 588, 180, 192), 245: (576, 588, 192, 204), 246: (576, 588, 204, 216), 247: (576, 588, 216, 228), 248: (576, 588, 228, 240), 249: (576, 588, 240, 252), 250: (576, 588, 252, 264), 251: (576, 588, 264, 276), 252: (588, 600, 0, 12), 253: (588, 600, 12, 24), 254: (588, 600, 24, 36), 255: (588, 600, 84, 96), 256: (588, 600, 120, 132), 257: (588, 600, 132, 144), 258: (588, 600, 144, 156), 259: (588, 600, 156, 168), 260: (588, 600, 168, 180), 261: (588, 600, 180, 192), 262: (588, 600, 192, 204), 263: (588, 600, 204, 216), 264: (588, 600, 216, 228), 265: (588, 600, 228, 240), 266: (588, 600, 240, 252), 267: (588, 600, 252, 264), 268: (588, 600, 264, 276), 269: (600, 612, 0, 12), 270: (600, 612, 12, 24), 271: (600, 612, 24, 36), 272: (600, 612, 36, 48), 273: (600, 612, 48, 60), 274: (600, 612, 60, 72), 275: (600, 612, 72, 84), 276: (600, 612, 84, 96), 277: (600, 612, 96, 108), 278: (600, 612, 108, 120), 279: (600, 612, 120, 132), 280: (600, 612, 132, 144), 281: (600, 612, 144, 156), 282: (600, 612, 156, 168), 283: (600, 612, 168, 180), 284: (600, 612, 180, 192), 285: (600, 612, 192, 204), 286: (600, 612, 204, 216), 287: (600, 612, 216, 228), 288: (600, 612, 228, 240), 289: (600, 612, 240, 252), 290: (600, 612, 252, 264), 291: (600, 612, 264, 276), 292: (600, 612, 276, 288)}\n",
      "labels:  [1 1 4 1 4 1 1 4 1 4 1 2 1 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 4 2 2 2 1\n",
      " 1 1 2 5 1 8 8 8 2 5 1 2 2 1 1 8 1 8 2 2 5 8 5 1 2 2 2 1 1 2 2 5 1 8 8 8 6\n",
      " 6 6 6 6 6 1 4 7 6 6 6 7 7 9 6 6 6 6 1 3 7 7 6 6 6 6 6 6 6 6 6 6 1 7 6 6 6\n",
      " 6 3 3 7 7 7 6 3 1 1 1 3 1 3 1 1 3 4 9 9 3 8 1 1 8 9 1 3 8 8 1 1 3 8 1 1 2\n",
      " 2 2 8 8 2 2 2 1 2 8 4 2 2 1 8 2 4 2 2 2 8 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2\n",
      " 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "X_kNN.shape:  (234, 309)\n",
      "Number of patches:  234\n",
      "X_kNN.shape:  (59, 309)\n",
      "Number of patches:  59\n",
      "Train Accuracy:  0.9184684882242007\n",
      "Test Accuracy:  0.764054661823214\n"
     ]
    }
   ],
   "source": [
    "# new version- dividing X to spectral bands\n",
    "whole_pipeline_divided_torch(torch.from_numpy(X),torch.from_numpy(y), rows_factor, cols_factor, is_normalize_each_band=True, method_label_patch='center')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    1    0    1    2\n",
      "   2    1    2    2    0\n",
      "   1    1    1    1    2\n",
      "   0    2    2    1    2\n",
      "   2    1    1    2    0\n",
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 2, 0],\n",
      "        [0, 1, 1, 2, 0],\n",
      "        [0, 2, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# #A test for overlapping patches:\n",
    "\n",
    "# import torch\n",
    "# from utils_torch import *\n",
    "\n",
    "# def print_tensor(tensor):\n",
    "#     for row in tensor:\n",
    "#         print(' '.join([str(elem.item()).rjust(4) for elem in row]))\n",
    "\n",
    "# rows = 5\n",
    "# cols = 5\n",
    "# data = torch.randint(0, 100, (rows, cols, 1), dtype=torch.int32)\n",
    "# labels = torch.randint(0, 3, (rows, cols), dtype=torch.int32)\n",
    "# print_tensor(labels)\n",
    "\n",
    "\n",
    "# factor = 5\n",
    "# overlap = 2\n",
    "# patched_data, patched_labels, labels = patch_data_overlap_torch(data, labels, factor, factor, overlap, overlap, method_label_patch=\"center\")\n",
    "\n",
    "# print(patched_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    1    2\n",
      "   0    1    2\n",
      "   1    0    2\n",
      "-----------------------\n",
      "   2    1    1    2\n",
      "   2    1    1    2\n",
      "   1    1    2    2\n",
      "   1    1    2    2\n",
      "-----------------------\n",
      "{0: (0, 2, 0, 2), 1: (0, 2, 1, 3), 2: (0, 2, 2, 4), 3: (0, 2, 3, 5), 4: (1, 3, 0, 2), 5: (1, 3, 1, 3), 6: (1, 3, 2, 4), 7: (1, 3, 3, 5), 8: (2, 4, 0, 2), 9: (2, 4, 1, 3), 10: (2, 4, 2, 4), 11: (2, 4, 3, 5), 12: (3, 5, 0, 2), 13: (3, 5, 1, 3), 14: (3, 5, 2, 4), 15: (3, 5, 3, 5)}\n",
      "-----------------------\n",
      "{(0, 0): [0], (0, 1): [0, 1], (1, 0): [0, 4], (1, 1): [0, 1, 4, 5], (0, 2): [1, 2], (1, 2): [1, 2, 5, 6], (0, 3): [2, 3], (1, 3): [2, 3, 6, 7], (0, 4): [3], (1, 4): [3, 7], (2, 0): [4, 8], (2, 1): [4, 5, 8, 9], (2, 2): [5, 6, 9, 10], (2, 3): [6, 7, 10, 11], (2, 4): [7, 11], (3, 0): [8, 12], (3, 1): [8, 9, 12, 13], (3, 2): [9, 10, 13, 14], (3, 3): [10, 11, 14, 15], (3, 4): [11, 15], (4, 0): [12], (4, 1): [12, 13], (4, 2): [13, 14], (4, 3): [14, 15], (4, 4): [15]}\n",
      "-----------------------\n",
      "{(0, 0): 2, (0, 1): 1, (1, 1): 1, (0, 2): 1, (1, 2): 1, (0, 3): 1, (1, 3): 1, (0, 4): 2, (2, 0): 1, (2, 2): 1, (2, 4): 2, (3, 0): 1, (3, 1): 1, (3, 2): 1, (3, 3): 2, (3, 4): 2, (4, 0): 1, (4, 1): 1, (4, 2): 1, (4, 3): 2}\n",
      "-----------------------\n",
      "{(1, 4): 2, (3, 0): 1, (3, 1): 1, (3, 2): 1, (4, 4): 2}\n"
     ]
    }
   ],
   "source": [
    "#A test for overlapping patches:\n",
    "\n",
    "import torch\n",
    "from utils_torch import *\n",
    "from classification import patch_to_points, point_to_patches, split_train_test\n",
    "\n",
    "def print_tensor(tensor):\n",
    "    for row in tensor:\n",
    "        print(' '.join([str(elem.item()).rjust(4) for elem in row]))\n",
    "\n",
    "rows = 3\n",
    "cols = 3\n",
    "data = torch.randint(0, 100, (rows, cols, 1), dtype=torch.int32)\n",
    "labels = torch.randint(0, 3, (rows, cols), dtype=torch.int32)\n",
    "print_tensor(labels)\n",
    "\n",
    "\n",
    "factor = 2\n",
    "overlap = 1\n",
    "patched_data, patched_labels, labels = patch_data_overlap_torch(data, labels, factor, factor, overlap, overlap)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print_tensor(patched_labels)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "dictionary = (patch_to_points(patched_labels.flatten(), factor, factor, overlap, overlap, num_patches_in_row=patched_labels.shape[1]))\n",
    "print(dictionary)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "dictionary = point_to_patches(dictionary)\n",
    "print(dictionary)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(split_train_test(dictionary, patched_labels.flatten())[1])\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(split_train_test(dictionary, patched_labels.flatten())[3])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
